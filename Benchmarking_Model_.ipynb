{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCeFoefbbFKb"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.37.2 --quiet\n",
        "!pip install torch --quiet\n",
        "!pip install tqdm boto3 requests regex sentencepiece sacremoses --quiet\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import tqdm, boto3, requests, regex, sentencepiece, sacremoses\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNpBsw31kogQ",
        "outputId": "c812c1b1-8e23-45f4-fb94-ba40b19f594b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub --quiet\n",
        "from huggingface_hub import PyTorchModelHubMixin\n",
        "\n",
        "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'yiyanghkust/finbert-tone')\n",
        "class BertForSequenceClassification(nn.Module, PyTorchModelHubMixin):\n",
        "    def __init__(self, pretrained_model_name, num_labels=3):\n",
        "        super(BertForSequenceClassification, self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.bert = torch.hub.load('huggingface/pytorch-transformers', 'model', pretrained_model_name)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "        self.dropout = nn.Dropout()\n",
        "        self.fc1 = nn.Linear(self.bert.config.hidden_size, 128)\n",
        "        self.finaloutput = nn.Linear(128, num_labels)\n",
        "        self.softmaxlayer = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels = None, *args, **kwargs):\n",
        "        #print(kwargs)\n",
        "        #print(args)\n",
        "        outputs = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        fc1_output = nn.functional.relu(self.fc1(pooled_output))\n",
        "        logits = self.finaloutput(fc1_output)\n",
        "        logits = self.softmaxlayer(logits)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "          loss = self.loss_fn(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "        return {\"logits\":logits,\n",
        "                \"loss\": loss\n",
        "                }\n",
        "\n",
        "def tokenize_function(examples):\n",
        "     return tokenizer(examples[\"text\"], padding = \"max_length\", truncation=True, max_length=128, return_tensors = \"pt\")\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"ANP1/finbert-tone-v0\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myinputs = [\"we don't expect a lot\",\n",
        "            \"growth is strong and will continue to be\",\n",
        "            \"the CEO had a meeting\",\n",
        "            \"stock market meltdown\"]\n",
        "myinputs = tokenizer(myinputs, return_tensors = 'pt', padding = \"max_length\", truncation=True, max_length=128)\n",
        "for row in model(**myinputs)[\"logits\"].detach().numpy():\n",
        "  print(np.argmax(row))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BaivcWv1rq4",
        "outputId": "77f8c250-6055-4354-d019-dddf10eb7735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "1\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr8ocnuZlA-8"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwDM6FF5YhPQpkGqkdyToh"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}